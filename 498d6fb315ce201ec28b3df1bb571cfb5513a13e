---
date: '2025-12-15'
deciders:
- '@zircote'
id: 20251215-use-swe-bench-lite-as-benchmark-format
linked_commits:
- be14872
status: accepted
tags:
- benchmark
- evaluation
title: Use SWE-bench Lite as Benchmark Format
---

# Use SWE-bench Lite as Benchmark Format

## Status

Accepted

## Context

We need to choose a benchmark format for evaluating Claude's software engineering capabilities. Options considered:

1. **SWE-bench Full** - Complete dataset with 2,294 tasks from 12 Python repositories
2. **SWE-bench Lite** - Curated subset with 300 well-scoped, self-contained issues
3. **Custom task format** - Our own schema inspired by SWE-bench methodology

## Decision

Use **SWE-bench Lite** (300 curated tasks) as the primary benchmark format.

## Consequences

### Positive
- Well-scoped tasks that are easier to run locally
- Curated for quality - each task has clear pass/fail criteria
- Direct comparability with published benchmarks
- Manageable size for iterative development
- Available via HuggingFace datasets API

### Negative
- Limited to 300 tasks vs full 2,294
- May not cover all edge cases
- Restricted to Python repositories only

### Neutral
- Can extend to SWE-bench Full later if needed
- Dataset schema is standardized and documented
